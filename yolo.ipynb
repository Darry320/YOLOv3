{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d77d79f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2105f8d5210>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pprint\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.cuda import amp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataloader import Dataset, AugmentTransform, BasicTransform\n",
    "from model import YoloModel\n",
    "from utils import (\n",
    "    YoloLoss, Evaluator, ModelEMA,\n",
    "    generate_random_color, set_lr, de_parallel,\n",
    "    resume_state\n",
    ")\n",
    "from val import validate, result_analyis\n",
    "\n",
    "\n",
    "SEED = 2025\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbe5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    exp = \"debug170multi\"  # 实验名称，会生成 experiment/debug/ 文件夹\n",
    "    data = Path(\"data/voc.yaml\")  # 数据配置文件路径\n",
    "    model_type = \"base\"  # YOLOv3 模型类型\n",
    "    img_size = 416  # 输入图像大小\n",
    "    batch_size = 8  # 批大小\n",
    "    num_epochs = 170  # 训练轮数\n",
    "    warmup = 1  # warmup epoch 数\n",
    "    base_lr = 0.001  # 初始学习率\n",
    "    lr_decay = [200]  # 指定在哪些 epoch 衰减学习率\n",
    "    momentum = 0.9  # SGD 动量\n",
    "    weight_decay = 5e-4  # 权重衰减\n",
    "    conf_thres = 0.01  # 置信度阈值（用于推理时过滤预测框）\n",
    "    nms_thres = 0.6  # NMS 阈值\n",
    "    img_interval = 10  # 每隔几轮保存一次图像 初始：5\n",
    "    workers = 4  # dataloader 多线程\n",
    "    multiscale = True  # 是否启用多尺度训练\n",
    "    no_amp = False  # 是否禁用混合精度\n",
    "    scratch = True  # 是否从头训练（不加载预训练权重）\n",
    "    resume = True  # 是否从上次 checkpoint 恢复\n",
    "    rank = 0  # 单卡训练设为 0\n",
    "    world_size = 1  # 单卡训练设为 1\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# 设置路径\n",
    "args.exp_path = Path(\"experiment\") / args.exp\n",
    "args.weight_dir = args.exp_path / \"weight\"\n",
    "args.img_log_dir = args.exp_path / \"train-image\"\n",
    "args.load_path = args.weight_dir / \"last.pt\" if args.resume else None\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(args.weight_dir, exist_ok=True)\n",
    "os.makedirs(args.img_log_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolov3_notebook(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loss_type = [\"multipart\", \"obj\", \"noobj\", \"txty\", \"twth\", \"cls\"]\n",
    "    losses = defaultdict(float)\n",
    "    \n",
    "\n",
    "    # 加载训练和验证集\n",
    "    train_dataset = Dataset(yaml_path=args.data, phase=\"train\")\n",
    "    val_dataset = Dataset(yaml_path=args.data, phase=\"val\")\n",
    "\n",
    "    # 记录 class 和 anchor 信息\n",
    "    args.anchors = train_dataset.anchors\n",
    "    args.class_list = train_dataset.class_list\n",
    "    args.color_list = generate_random_color(len(args.class_list))\n",
    "    args.mAP_filepath = val_dataset.mAP_filepath\n",
    "\n",
    "    # 设置数据增强器\n",
    "    args.train_size = 608 if args.multiscale else args.img_size\n",
    "    train_dataset.load_transformer(AugmentTransform(input_size=args.train_size))\n",
    "    train_loader = DataLoader(train_dataset, collate_fn=Dataset.collate_fn,\n",
    "                              batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    val_dataset.load_transformer(BasicTransform(input_size=args.img_size))\n",
    "    val_loader = DataLoader(val_dataset, collate_fn=Dataset.collate_fn,\n",
    "                            batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    # 计算 warmup 阶段和梯度累计参数\n",
    "    args.nw = max(round(args.warmup * len(train_loader)), 100)\n",
    "    args.nominal_batch_size = 64\n",
    "    args.grad_accumulate = max(round(args.nominal_batch_size / args.batch_size), 1)\n",
    "    args.last_opt_step = -1\n",
    "\n",
    "    # 初始化模型、损失函数、优化器等\n",
    "    model = YoloModel(args.img_size, len(args.class_list), args.anchors,\n",
    "                      args.model_type, pretrained=not args.scratch).to(device)\n",
    "    model.set_grid_xy(input_size=args.train_size)\n",
    "\n",
    "    criterion = YoloLoss(args.train_size, len(args.class_list), anchors=model.anchors)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.base_lr,\n",
    "                          momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_decay, gamma=0.1)\n",
    "    scaler = amp.GradScaler(enabled=not args.no_amp)\n",
    "    ema = ModelEMA(model)\n",
    "    evaluator = Evaluator(annotation_file=args.mAP_filepath)\n",
    "    \n",
    "\n",
    "    # 恢复训练\n",
    "    start_epoch = 1\n",
    "    if args.resume:\n",
    "        start_epoch = resume_state(args.load_path, args.rank, model, ema, optimizer, scheduler, scaler)\n",
    "\n",
    "    # ---------- 开始训练循环 ----------\n",
    "    for epoch in range(start_epoch, args.num_epochs + 1):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{args.num_epochs}\", leave=False)\n",
    "        \n",
    "        # epoch_loss = defaultdict(float)\n",
    "\n",
    "        for i, minibatch in enumerate(loop):\n",
    "            ni = i + len(train_loader) * (epoch - 1)\n",
    "\n",
    "            # 动态调整学习率 & 梯度累计\n",
    "            if ni <= args.nw:\n",
    "                args.grad_accumulate = max(1, np.interp(ni, [0, args.nw],\n",
    "                                                        [1, args.nominal_batch_size / args.batch_size]).round())\n",
    "                set_lr(optimizer, args.base_lr * pow(ni / args.nw, 4))\n",
    "\n",
    "            images, labels = minibatch[1].to(device, non_blocking=True), minibatch[2]\n",
    "\n",
    "            # 多尺度训练（随机 resize）\n",
    "            if args.multiscale:\n",
    "                if ni % 10 == 0 and ni > 0:\n",
    "                    args.train_size = random.randint(10, 19) * 32\n",
    "                    model.module.set_grid_xy(input_size=args.train_size) if hasattr(model, \"module\") else model.set_grid_xy(input_size=args.train_size)\n",
    "                    criterion.set_grid_xy(input_size=args.train_size)\n",
    "                images = nn.functional.interpolate(images, size=args.train_size, mode=\"bilinear\")\n",
    "\n",
    "            # 混合精度前向 & 计算 loss\n",
    "            with amp.autocast(enabled=not args.no_amp):\n",
    "                predictions = model(images)\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "            # 反向传播（支持梯度累计）\n",
    "            scaler.scale((loss[0] / args.grad_accumulate)).backward()\n",
    "\n",
    "            # 执行优化器 step\n",
    "            if ni - args.last_opt_step >= args.grad_accumulate:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "                args.last_opt_step = ni\n",
    "                \n",
    "            for loss_name, loss_value in zip(loss_type, loss):\n",
    "                if not torch.isfinite(loss_value) and loss_name != \"multipart\":\n",
    "                    print(f\"############## {loss_name} Loss is Nan/Inf ! {loss_value} ##############\")\n",
    "                    sys.exit(0)\n",
    "                else:\n",
    "                    losses[loss_name] += loss_value.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch} completed\")\n",
    "\n",
    "        # ---------- 验证 & 评估 ----------\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            mAP_dict, eval_text = validate(args=args, dataloader=val_loader, model=ema.module, evaluator=evaluator, epoch=epoch)\n",
    "            if mAP_dict:\n",
    "                print(eval_text)\n",
    "        \n",
    "        \n",
    "        # ---------- 保存模型 ----------\n",
    "        ckpt = {\n",
    "            \"running_epoch\": epoch,\n",
    "            \"model_type\": args.model_type,\n",
    "            \"class_list\": args.class_list,\n",
    "            \"anchors\": args.anchors,\n",
    "            \"model_state\": de_parallel(model).state_dict(),\n",
    "            \"ema_state\": ema.module.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict(),\n",
    "            \"scaler_state_dict\": scaler.state_dict(),\n",
    "        }\n",
    "        torch.save(ckpt, args.weight_dir / \"last.pt\")\n",
    "\n",
    "        # ---------- 可视化 & 分析 ----------\n",
    "        if epoch % 10 == 0:\n",
    "            if mAP_dict:\n",
    "                result_analyis(args=args, mAP_dict=mAP_dict[\"all\"])\n",
    "                \n",
    "    loss_str = f\"[Train-Epoch:{epoch:03d}] \"\n",
    "    for loss_name in loss_type:\n",
    "        losses[loss_name] /= len(loop)\n",
    "        loss_str += f\"{loss_name}: {losses[loss_name]:.4f}  \"\n",
    "    return loss_str\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb43d421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170 completed\n",
      "\n",
      "\t - Average Precision (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.055\n",
      "\t - Average Precision (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.210\n",
      "\t - Average Precision (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.008\n",
      "\t - Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      "\t - Average Precision (AP) @[ IoU=0.50      | area= small | maxDets=100 ] = 0.000\n",
      "\t - Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      "\t - Average Precision (AP) @[ IoU=0.50      | area=medium | maxDets=100 ] = 0.000\n",
      "\t - Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.074\n",
      "\t - Average Precision (AP) @[ IoU=0.50      | area= large | maxDets=100 ] = 0.282\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_yolov3_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 136\u001b[0m, in \u001b[0;36mtrain_yolov3_notebook\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    134\u001b[0m loss_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Train-Epoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss_name \u001b[38;5;129;01min\u001b[39;00m loss_type:\n\u001b[1;32m--> 136\u001b[0m     losses[loss_name] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdataloader\u001b[49m)\n\u001b[0;32m    137\u001b[0m     loss_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[loss_name]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_str\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "train_yolov3_notebook(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
